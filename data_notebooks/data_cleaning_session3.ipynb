{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session 3 - Data cleaning\n",
    "\n",
    "Today we're going to look at how to clean data using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# First lets read in some movie metadata that we know needs cleaning. \n",
    "# This dataset is from Kaggle (https://www.kaggle.com/datasets/carolzhangdc/imdb-5000-movie-dataset)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('data/movie_metadata_unclean.csv')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Let's examine our data\n",
    "\n",
    "\n",
    "1. When we look at the dataset  we can start to note down the problems, and then we’ll come up with solutions to fix those problems.\n",
    "\n",
    "2. Pandas has some selection methods which we can use to slice and dice the dataset based on your queries.\n",
    "\n",
    "EXAMPLES:\n",
    "\n",
    "- Look at the some basic stats for the ‘imdb_score’ column: `data.imdb_score.describe()`\n",
    "- Select a column: `data[‘movie_title’]`\n",
    "- Select the first 10 rows of a column: `data[‘duration’][:10]`\n",
    "- Select multiple columns: `data[[‘budget’,’gross’]]`\n",
    "- Select all movies over two hours long: `data[data[‘duration’] > 120]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.imdb_score.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['movie_title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['duration'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['budget','gross']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['duration'] > 120]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Missing Data\n",
    "\n",
    "One of the most common problems is missing data. This could be because it was never filled out properly, the data wasn’t available, or there was a computing error. \n",
    "\n",
    "> __IMPORTANT: if we leave the blank values in there, it will cause errors in analysis later on, so we need to process the data to deal with missing values__\n",
    "\n",
    "OPTIONS:\n",
    "\n",
    "- Add in a default value for the missing data\n",
    "- Get rid of (delete) the rows that have missing data\n",
    "- Get rid of (delete) the columns that have a high incidence of missing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how many non-null values in each column\n",
    "len(data) - data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Add default values\n",
    "\n",
    "- Let's get rid of all those nasty NaN values. \n",
    "- QUESTION: what to put in its place? This is where we need to jugde the dataset and make an executive decision!\n",
    "\n",
    "For our example, let’s look at the ‘country’ column. It’s straightforward enough, but some of the movies don’t have a country provided so the data shows up as NaN. In this case, we probably don’t want to assume the country, so we can replace it with an __empty string__ or some other default value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.country.isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see which films don't have a country\n",
    "data[['movie_title', 'country']][data.country.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# This replaces the NaN entries in the ‘country’ column with the empty string, \n",
    "# but we could just as easily tell it to replace with a default name such as “Not known” or \"Other\".\n",
    "\n",
    "data.country = data.country.fillna('')\n",
    "\n",
    "data.loc[[4]][['movie_title', 'country']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's look at a numeric column - duration\n",
    "\n",
    "data[['movie_title', 'duration']][data.duration.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# With numerical data like the \"duration\" of the movie, \n",
    "# a calculation like taking the *mean duration* can help us even the dataset out. \n",
    "\n",
    "# That way we don’t have crazy numbers like 0 or NaN throwing off our analysis.\n",
    "\n",
    "data.duration = data.duration.fillna(data.duration.mean())\n",
    "\n",
    "data.loc[[4]][['movie_title', 'duration']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Remove incomplete rows\n",
    "\n",
    "- Now we want to get rid of any rows that have a missing value. \n",
    "- It’s a pretty aggressive technique, but there may be a use case where that’s exactly what __we want to do.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's create a copy of the data set to play with and count the rows\n",
    "test_data = data.copy()\n",
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test dropping all rows with any NA values:\n",
    "test_data = data.copy()\n",
    "\n",
    "test_data.dropna(inplace=True)\n",
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# We can also drop rows that have ALL NA values (which we don't have any of):\n",
    "test_data = data.copy()\n",
    "\n",
    "test_data.dropna(how='all', inplace=True)\n",
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Put a limitation on how many non-null values need to be in a row in order to keep it \n",
    "# (in this example, the data needs to have at least 25 non-null values):\n",
    "test_data = data.copy()\n",
    "\n",
    "test_data.dropna(thresh=25, inplace=True)\n",
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# In this instance that we don’t want to include any movie \n",
    "# that doesn’t have information on when the movie came out:\n",
    "test_data = data.copy()\n",
    "\n",
    "test_data.dropna(subset=['title_year'], inplace=True)\n",
    "test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Dealing  with error-prone columns\n",
    "\n",
    "- We can apply the same kind of criteria to our __columns.__ \n",
    "- But we just need to use the parameter __axis=1__ in our code. \n",
    "- That means to operate on columns, not rows. \n",
    "\n",
    "> _Do not run the code below if you do not want to delete data - otherwise feel free to experiment!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Drop the columns with that are all NA values (we don't have any of these):\n",
    "test_data = data.copy()\n",
    "\n",
    "test_data.dropna(axis=1, how='all', inplace=True)\n",
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Drop all columns with *any* NA values:\n",
    "test_data = data.copy()\n",
    "\n",
    "test_data.dropna(axis=1, how='any', inplace=True)\n",
    "test_data.shape\n",
    "\n",
    "# Note: we can use same threshold and subset params as we did with rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Normalize data types\n",
    "\n",
    "- Sometimes, especially when we are reading in a CSV with a bunch of numbers, some of the numbers will read in as __strings__ instead of numeric values, or vice versa.\n",
    "\n",
    "- Let's review a couple of ways to fix and normlise our data types.\n",
    "\n",
    "- Please note that we are going to read data from disk again, so the types are converted on data rparsing\n",
    "\n",
    "- If we just run the next line of code it owuld throw an error! Complaining about NaN values. We need to save our previous results into a file and then read the file again. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# 1. save results into the file again\n",
    "\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "data.to_csv('data/movie_metadata_cleaned.csv')\n",
    "\n",
    "# 2. read from the file again\n",
    "\n",
    "data = pd.read_csv('data/movie_metadata_cleaned.csv')\n",
    "\n",
    "# Look at how the duration field has been read in (float)\n",
    "data.duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now force it to be an integer\n",
    "data = pd.read_csv('data/movie_metadata_cleaned.csv', dtype={'duration': int})\n",
    "data.duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Same with actor_2_facebook_likes field\n",
    "\n",
    "data = pd.read_csv('data/movie_metadata_cleaned.csv')\n",
    "data.actor_2_facebook_likes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force actor_2_facebook_likes to be a string\n",
    "\n",
    "data = pd.read_csv('data/movie_metadata_cleaned.csv', dtype={'actor_2_facebook_likes': str})\n",
    "data.actor_2_facebook_likes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Change casing\n",
    "\n",
    "- Columns with user-provided data are ripe for corruption. \n",
    "- People make typos, leave their caps lock on (or off), and add extra spaces where they shouldn’t.\n",
    "- Let's see how to correct these issues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "data['movie_title'].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#  Let's get rid of trailing whitespace\n",
    "\n",
    "data['movie_title'].str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DID YOU KNOW:  \n",
    "\n",
    "### It is also possible to correct spelling mistakes in your data!\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "- We will not be covering this in our course, but you can read about it in your spare time\n",
    "- It is called __FUZZY MATCHING__ \n",
    "- Fuzzy string matching uses __[Levenshtein Distance] (https://en.wikipedia.org/wiki/Levenshtein_distance)__ to calculate the differences between sequences\n",
    "- note the exclamation sign below\n",
    "    \n",
    "</div>\n",
    "\n",
    "<img src=\"data/images/fuzzy.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Rename columns\n",
    "\n",
    "- If your data was generated by a computer program, it probably has some computer-generated column names too. \n",
    "- Those can be hard to read and understand while working\n",
    "- We can rename a column to something more user-friendly\n",
    "- we have already practiced that in earlier pandas tutorial, let's remind ourselves how to do it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "data.rename(columns = {'title_year':'release_date', 'movie_facebook_likes':'facebook_likes'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Saving Results\n",
    "\n",
    "- When you’re done cleaning your data, you may want to export it back into CSV format for further processing in another program.\n",
    "- Always remember to save your data, otherwise all our efforts would be lost. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "data.to_csv('data/movie_metadata_cleaned.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NON-CODING DEMO SLIDES\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "- There are many advanced techniques on how you can clean, inspect, process data\n",
    "\n",
    "- <b>We won't do any coding</b>, but we will have a high level walk through to review some of them\n",
    "\n",
    "- In the future you can explore these techniques in more detail\n",
    "</div>\n",
    "\n",
    "\n",
    "## Let's review how to identify and see:\n",
    "\n",
    "- ##### Missing Data\n",
    "- ##### Irregular Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Technique 1: Missing Data Heatmap\n",
    "\n",
    "- When there is a smaller number of features, we can visualize the missing data via heatmap.\n",
    "- The chart below demonstrates the missing data patterns of the first 30 features. \n",
    "    - The horizontal axis shows the feature name; \n",
    "    - the vertical axis shows the number of observations/rows; \n",
    "    - the yellow color represents the missing data while the blue color otherwise.\n",
    "    \n",
    "    \n",
    "<img src=\"data/images/missing1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Technique 2: Missing Data Percentage List\n",
    "\n",
    "- When there are many features in the dataset, we can make a list of missing data % for each feature.\n",
    "- The list below shows the percentage of missing values for each of the features.\n",
    "\n",
    "<img src=\"data/images/missing2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Technique 3: Missing Data Histogram\n",
    "\n",
    "- Missing data histogram is also a technique for when we have many features.\n",
    "- To learn more about the missing value patterns among observations, we can visualize it by a histogram.\n",
    "- This histogram helps to identify the missing values situations among the 30,471 observations.\n",
    "\n",
    "<img src=\"data/images/missing3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Irregular data (Outliers)\n",
    "\n",
    "- Outliers are data that is distinctively different from other observations. \n",
    "- They could be real outliers or mistakes.\n",
    "- Depending on whether the feature is numeric or categorical, we can use different techniques to study its distribution to detect outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Technique 1: Histogram/Box Plot\n",
    "\n",
    "- When the feature is numeric, we can use a histogram and box plot to detect outliers.\n",
    "- The data looks highly skewed with the possible existence of outliers.\n",
    "\n",
    "<img src=\"data/images/outlier1.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Technique 2: Bar Chart\n",
    "\n",
    "- When the feature is categorical, we can use a bar chart to learn about its categories and distribution.\n",
    "- For example, the feature ecology has a reasonable distribution. \n",
    "- But if there is a category with only one value called “other”, then that would be an outlier.\n",
    "\n",
    "<img src=\"data/images/outlier2.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Unnecessary data\n",
    "\n",
    "- All the data feeding into the model should serve the purpose of the project. \n",
    "- The unnecessary data is when the data doesn’t add value. \n",
    "- We cover two main types of unnecessary data due to different reasons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Unnecessary type 1: Uninformative / Repetitive\n",
    "\n",
    "- Sometimes one feature is uninformative because it has too many rows being the same value.\n",
    "- We can create a list of features with a high percentage of the same value.\n",
    "- For example, we specify below to show features with over 95% rows being the same value.\n",
    "\n",
    "<img src=\"data/images/unnecessary1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Unnecessary type 2: Duplicates\n",
    "\n",
    "- The duplicate data is when copies of the same observation exist.\n",
    "- There are two main types of duplicate data\n",
    "\n",
    "__1. Duplicates type 1: All Features based__\n",
    "\n",
    "- This duplicate happens when all the features’ values within the observations are the same. \n",
    "\n",
    "__2.Duplicates type 2: Key Features based__\n",
    "\n",
    "- Sometimes it is better to remove duplicate data based on a set of unique identifiers.\n",
    "- For example, the chances of two transactions happening at the same time, with the same square footage, the same price, and the same build year are close to zero.\n",
    "- We can set up a group of critical features as unique identifiers for transactions and we check if there are duplicates based on them.\n",
    "\n",
    "> We will need to identiy and remove those duplicates (if necessary)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
